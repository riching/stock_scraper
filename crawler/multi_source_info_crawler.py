#!/usr/bin/env python3
"""å¤šæºè‚¡ç¥¨ä¿¡æ¯çˆ¬å–å™¨ï¼Œä½¿ç”¨Playwrightæå–çœŸå®æ•°æ®"""

import sys
import os
import asyncio
import json
import sqlite3
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any

from utils.deduplication import DeduplicationManager
from utils.incremental_crawler import IncrementalCrawler
from utils.data_saver import save_stock_info_data
from utils.sentiment_calculator import calculate_overall_sentiment_score
from crawler.common.llm_analyzer import LLMAnalyzer
from crawler.common.extractors_new import (
    SinaFinanceNewsExtractor,
    EastMoneyAnnouncementExtractor,
    TonghuashunNewsExtractor,
    TonghuashunAnnouncementExtractor,
    XueqiuNewsExtractor,
    XueqiuCommentExtractor,
)
from utils.monitor_manager import MonitorManager


class MultiSourceInfoCrawler:
    """å¤šæºè‚¡ç¥¨ä¿¡æ¯çˆ¬å–ç³»ç»Ÿä¸»ç±»"""

    def __init__(self, db_path: str, progress_file: str = "crawl_progress.json"):
        self.db_path = db_path
        self.dedup_manager = DeduplicationManager(db_path)
        self.incremental_crawler = IncrementalCrawler(db_path)
        self.llm_analyzer = LLMAnalyzer()
        self.monitor_manager = MonitorManager(db_path, progress_file)
        self.three_months_ago = (datetime.now() - timedelta(days=90)).strftime(
            "%Y-%m-%d"
        )

        self.sina_news_extractor = SinaFinanceNewsExtractor()
        self.eastmoney_announcement_extractor = EastMoneyAnnouncementExtractor()
        self.tonghuashun_news_extractor = TonghuashunNewsExtractor()
        self.tonghuashun_announcement_extractor = TonghuashunAnnouncementExtractor()
        self.xueqiu_news_extractor = XueqiuNewsExtractor()
        self.xueqiu_comment_extractor = XueqiuCommentExtractor()

    async def crawl_stock_info(self, stock_code: str, page) -> Optional[float]:
        """çˆ¬å–å•ä¸ªè‚¡ç¥¨çš„å¤šç»´åº¦ä¿¡æ¯"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–è‚¡ç¥¨ {stock_code} çš„ä¿¡æ¯...")

        existing_score = self.incremental_crawler.get_default_score_for_stock(
            stock_code
        )
        if existing_score != -1.0:
            print(f"â­ï¸  è‚¡ç¥¨ {stock_code} å·²æœ‰è¿‘æœŸè¯„åˆ†: {existing_score:.2f}")
            return existing_score

        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                all_news_data = []
                all_announcement_data = []
                all_comment_data = []
                all_report_data = []

                print(f"ğŸ“° æå–å¤šæºæ–°é—»æ•°æ®...")

                try:
                    sina_news = await self.sina_news_extractor.extract_news(
                        page, stock_code
                    )
                    if sina_news:
                        all_news_data.extend(sina_news)
                    print(f"âœ… æ–°æµªè´¢ç»æ–°é—»: {len(sina_news)} æ¡")
                except Exception as e:
                    print(f"âš ï¸  æ–°æµªè´¢ç»æ–°é—»æå–å¤±è´¥: {e}")

                try:
                    xueqiu_news = await self.xueqiu_news_extractor.extract_news(
                        page, stock_code
                    )
                    if xueqiu_news:
                        all_news_data.extend(xueqiu_news)
                    print(f"âœ… é›ªçƒæ–°é—»: {len(xueqiu_news)} æ¡")
                except Exception as e:
                    print(f"âš ï¸  é›ªçƒæ–°é—»æå–å¤±è´¥: {e}")

                try:
                    tonghuashun_news = (
                        await self.tonghuashun_news_extractor.extract_news(stock_code)
                    )
                    if tonghuashun_news:
                        all_news_data.extend(tonghuashun_news)
                    print(f"âœ… åŒèŠ±é¡ºæ–°é—»: {len(tonghuashun_news)} æ¡")
                except Exception as e:
                    print(f"âš ï¸  åŒèŠ±é¡ºæ–°é—»æå–å¤±è´¥: {e}")

                print(f"ğŸ“¢ æå–å¤šæºå…¬å‘Šæ•°æ®...")

                try:
                    eastmoney_announcements = await self.eastmoney_announcement_extractor.extract_announcements(
                        stock_code
                    )
                    if eastmoney_announcements:
                        all_announcement_data.extend(eastmoney_announcements)
                    print(f"âœ… ä¸œæ–¹è´¢å¯Œå…¬å‘Š: {len(eastmoney_announcements)} æ¡")
                except Exception as e:
                    print(f"âš ï¸  ä¸œæ–¹è´¢å¯Œå…¬å‘Šæå–å¤±è´¥: {e}")

                try:
                    tonghuashun_announcements = await self.tonghuashun_announcement_extractor.extract_announcements(
                        stock_code
                    )
                    if tonghuashun_announcements:
                        all_announcement_data.extend(tonghuashun_announcements)
                    print(f"âœ… åŒèŠ±é¡ºå…¬å‘Š: {len(tonghuashun_announcements)} æ¡")
                except Exception as e:
                    print(f"âš ï¸  åŒèŠ±é¡ºå…¬å‘Šæå–å¤±è´¥: {e}")

                print(f"ğŸ’¬ æå–å¤šæºè¯„è®ºæ•°æ®...")

                try:
                    xueqiu_comments = (
                        await self.xueqiu_comment_extractor.extract_comments(
                            page, stock_code
                        )
                    )
                    if xueqiu_comments:
                        all_comment_data.extend(xueqiu_comments)
                    print(f"âœ… é›ªçƒè¯„è®º: {len(xueqiu_comments)} æ¡")
                except Exception as e:
                    print(f"âš ï¸  é›ªçƒè¯„è®ºæå–å¤±è´¥: {e}")

                unique_news = self._filter_unique_items(all_news_data, "news")
                unique_announcements = self._filter_unique_items(
                    all_announcement_data, "announcement"
                )
                unique_comments = self._filter_unique_items(all_comment_data, "comment")

                analyzed_news = await self._analyze_content_batch(unique_news, "news")
                analyzed_announcements = await self._analyze_content_batch(
                    unique_announcements, "announcement"
                )
                analyzed_comments = await self._analyze_content_batch(
                    unique_comments, "comment"
                )

                if analyzed_news:
                    save_stock_info_data(self.db_path, "stock_news", analyzed_news)
                if analyzed_announcements:
                    save_stock_info_data(
                        self.db_path, "stock_announcements", analyzed_announcements
                    )
                if analyzed_comments:
                    save_stock_info_data(
                        self.db_path, "stock_comments", analyzed_comments
                    )

                news_score = self._calculate_average_score(analyzed_news)
                announcement_score = self._calculate_average_score(
                    analyzed_announcements
                )
                comment_score = self._calculate_average_score(analyzed_comments)
                report_score = None

                overall_score = calculate_overall_sentiment_score(
                    news_score, announcement_score, comment_score, report_score
                )

                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                try:
                    cursor.execute(
                        """
                        INSERT OR REPLACE INTO stock_sentiment_scores 
                        (code, date, news_score, announcement_score, comment_score, analyst_score, overall_score, analysis_summary)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            stock_code,
                            datetime.now().strftime("%Y-%m-%d"),
                            news_score,
                            announcement_score,
                            comment_score,
                            report_score,
                            overall_score,
                            f"å¤šæºçˆ¬å–: æ–°é—»{len(analyzed_news)}æ¡, å…¬å‘Š{len(analyzed_announcements)}æ¡, è¯„è®º{len(analyzed_comments)}æ¡, é‡è¯•{retry_count}æ¬¡",
                        ),
                    )
                    conn.commit()
                    print(f"âœ… è‚¡ç¥¨ {stock_code} ç»¼åˆè¯„åˆ†: {overall_score:.2f}/10")
                except Exception as e:
                    print(f"âŒ ä¿å­˜ç»¼åˆè¯„åˆ†å¤±è´¥: {e}")
                    return None
                finally:
                    conn.close()

                self.incremental_crawler.update_crawl_status(
                    stock_code, "news", len(analyzed_news)
                )
                self.incremental_crawler.update_crawl_status(
                    stock_code, "announcement", len(analyzed_announcements)
                )
                self.incremental_crawler.update_crawl_status(
                    stock_code, "comment", len(analyzed_comments)
                )

                print(f"âœ… è‚¡ç¥¨ {stock_code} ä¿¡æ¯çˆ¬å–å®Œæˆï¼")
                return overall_score

            except Exception as e:
                print(
                    f"âŒ è‚¡ç¥¨ {stock_code} çˆ¬å–å¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries}): {e}"
                )
                if retry_count < max_retries - 1:
                    retry_count += 1
                    await asyncio.sleep(5 * (retry_count + 1))
                    continue
                else:
                    return None

        return None

    def _filter_unique_items(self, items: List[Dict], content_type: str) -> List[Dict]:
        """è¿‡æ»¤é‡å¤å†…å®¹"""
        unique_items = []
        seen_fingerprints = set()
        for item in items:
            if item["fingerprint"] not in seen_fingerprints:
                seen_fingerprints.add(item["fingerprint"])
                unique_items.append(item)
        return unique_items

    async def _analyze_content_batch(
        self, items: List[Dict], content_type: str
    ) -> List[Dict]:
        """æ‰¹é‡åˆ†æå†…å®¹"""
        analyzed_items = []
        for item in items:
            if not self.dedup_manager.is_content_exists(
                item["fingerprint"], f"stock_{content_type}s"
            ):
                analysis = self.llm_analyzer.analyze_content(
                    item["content"], content_type
                )
                item.update(
                    {
                        "sentiment_score": analysis["sentiment_score"],
                        "is_valid": analysis["is_valid"],
                        "llm_analysis": json.dumps(analysis),
                    }
                )
                analyzed_items.append(item)
        return analyzed_items

    def _calculate_average_score(self, items: List[Dict]) -> Optional[float]:
        """è®¡ç®—å¹³å‡åˆ†"""
        valid_scores = [
            item["sentiment_score"] for item in items if item.get("is_valid", False)
        ]
        if valid_scores:
            return sum(valid_scores) / len(valid_scores)
        return None

    async def run_full_crawl(self, batch_size: int = 50):
        """è¿è¡Œå…¨é‡çˆ¬å–ä»»åŠ¡"""
        print("ğŸ¯ å¼€å§‹å…¨é‡çˆ¬å–ä»»åŠ¡...")
        print("=" * 60)

        stocks_to_process = self.monitor_manager.get_stocks_to_process(batch_size)
        if not stocks_to_process:
            print("ğŸ“­ æ‰€æœ‰è‚¡ç¥¨éƒ½å·²å¤„ç†è¿‡ï¼Œæ£€æŸ¥ç°æœ‰ç»“æœ...")
            high_scoring_stocks = self.monitor_manager.get_high_scoring_stocks()
            self.monitor_manager.save_results_to_file(high_scoring_stocks)
            return high_scoring_stocks

        print(
            f"ğŸ“Š éœ€è¦å¤„ç† {len(stocks_to_process)} åªè‚¡ç¥¨: {stocks_to_process[:10]}{'...' if len(stocks_to_process) > 10 else ''}"
        )

        from playwright.async_api import async_playwright

        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-gpu",
                    "--single-process",
                ],
            )

            try:
                page = await browser.new_page()
                await page.set_extra_http_headers(
                    {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    }
                )

                scores = {}
                for stock_code in stocks_to_process:
                    try:
                        score = await self.crawl_stock_info(stock_code, page)
                        if score is not None:
                            scores[stock_code] = score
                            self.monitor_manager.mark_stock_success(stock_code, score)
                        else:
                            scores[stock_code] = -1.0
                            self.monitor_manager.mark_stock_failed(
                                stock_code, "çˆ¬å–å¤±è´¥"
                            )

                        progress = self.monitor_manager.get_progress_summary()
                        print(f"ğŸ“ˆ è¿›åº¦: {progress}")

                        await asyncio.sleep(2)

                    except Exception as e:
                        print(f"âŒ è‚¡ç¥¨ {stock_code} å¤„ç†å¼‚å¸¸: {e}")
                        scores[stock_code] = -1.0
                        self.monitor_manager.mark_stock_failed(stock_code, str(e))

                await browser.close()
                await self.eastmoney_announcement_extractor.close_session()
                await self.tonghuashun_news_extractor.close_session()
                await self.tonghuashun_announcement_extractor.close_session()

                print("=" * 60)
                print(f"ğŸ‰ æ‰¹æ¬¡å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(stocks_to_process)} åªè‚¡ç¥¨")
                print(
                    f"ğŸ“Š æˆåŠŸ: {len([s for s in scores.values() if s != -1.0])}, å¤±è´¥: {len([s for s in scores.values() if s == -1.0])}"
                )

                high_scoring_stocks = self.monitor_manager.get_high_scoring_stocks()
                self.monitor_manager.save_results_to_file(high_scoring_stocks)

                return high_scoring_stocks

            except Exception as e:
                print(f"âŒ æµè§ˆå™¨æ“ä½œå¤±è´¥: {e}")
                await browser.close()
                raise


async def main():
    """ä¸»å‡½æ•°"""
    db_path = "/Users/riching/work/hywork/db/sqlite/full_a_stock_cache.db"
    crawler = MultiSourceInfoCrawler(db_path)

    print("ğŸš€ å¼€å§‹å…¨é‡æ•°æ®çˆ¬å–...")
    try:
        high_scoring_stocks = await crawler.run_full_crawl(batch_size=50)
        print(f"\nğŸ‰ å…¨é‡çˆ¬å–å®Œæˆï¼æ‰¾åˆ° {len(high_scoring_stocks)} åªé«˜è¯„åˆ†è‚¡ç¥¨")

        print("\nğŸ† å‰10åè‚¡ç¥¨:")
        for i, stock in enumerate(high_scoring_stocks[:10], 1):
            if stock["score"] == -1.0:
                print(f"{i:2d}. {stock['code']} - é»˜è®¤è¯„åˆ†(-1)")
            else:
                print(f"{i:2d}. {stock['code']} - {stock['score']:.2f}/10")

    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
        pass
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
