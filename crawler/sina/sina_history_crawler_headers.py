#!/usr/bin/env python3
"""ä¼˜åŒ–åçš„æ–°æµªè´¢ç»å†å²æ•°æ®çˆ¬è™« - ä½¿ç”¨å®Œæ•´çš„é¡µé¢è¯·æ±‚headers"""

import asyncio
import sys
import os
import json
import re
import requests
from datetime import datetime
from typing import Dict, Optional
from playwright.async_api import async_playwright

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from crawler.sina.market_prefix_helper import get_market_prefix


class SinaHistoryFetcher:
    """æ–°æµªè´¢ç»å†å²æ•°æ®è·å–å™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self):
        self.name = "SinaHistoryFetcher"
        self.session = requests.Session()
        self.cookies = {}
        self.headers = {}
        self.session_initialized = False
    
    async def init_session(self, stock_code: str):
        """åˆå§‹åŒ–ä¼šè¯ï¼šè®¿é—®é¡µé¢è·å–cookieså’Œæ‰€æœ‰headers"""
        if self.session_initialized:
            return True
        
        try:
            print(f"  ğŸŒ åˆå§‹åŒ–ä¼šè¯: {stock_code}")
            
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(
                headless=True,
                args=[
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                    "--disable-dev-shm-usage",
                ]
            )
            
            context = await browser.new_context()
            page = await context.new_page()
            
            # æ”¶é›†é¡µé¢è¯·æ±‚çš„headers
            page_headers = {}
            
            async def handle_request(request):
                url = request.url
                if "finance.sina.com.cn" in url and "realstock" in url:
                    page_headers.update(dict(request.headers))
            
            page.on("request", handle_request)
            
            # è®¿é—®é¡µé¢
            market_prefix = get_market_prefix(stock_code)
            url = f"http://finance.sina.com.cn/realstock/company/{market_prefix}{stock_code}/nc.shtml"
            
            print(f"  ğŸ“„ è®¿é—®é¡µé¢: {url}")
            await page.goto(url, wait_until="domcontentloaded", timeout=20000)
            await page.wait_for_timeout(3000)
            
            # è·å–cookies
            cookies = await context.cookies()
            print(f"  ğŸª è·å–åˆ° {len(cookies)} ä¸ªcookies")
            
            # è½¬æ¢cookiesä¸ºrequestsæ ¼å¼
            for cookie in cookies:
                self.cookies[cookie['name']] = cookie['value']
            
            # è·å–é¡µé¢è¯·æ±‚çš„headers
            if page_headers:
                print(f"  ğŸ“‹ è·å–åˆ°é¡µé¢è¯·æ±‚headers: {len(page_headers)} ä¸ª")
                self.headers = page_headers
            else:
                print(f"  âš ï¸  æœªè·å–åˆ°é¡µé¢è¯·æ±‚headersï¼Œä½¿ç”¨é»˜è®¤headers")
                self.headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Referer': 'http://finance.sina.com.cn/',
                    'Accept': 'application/json, text/javascript, */*; q=0.01',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                }
            
            await browser.close()
            await playwright.stop()
            
            self.session_initialized = True
            print(f"  âœ… ä¼šè¯åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"  âŒ ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def fetch_history_data(self, stock_code: str, target_date: str) -> Optional[Dict]:
        """ä»APIè·å–å†å²æ•°æ®"""
        try:
            market_code = get_market_prefix(stock_code)
            api_url = "http://money.finance.sina.com.cn/quotes_service/api/json_v2.php/CN_MarketData.getKLineData"
            
            params = {
                'symbol': f'{market_code}{stock_code}',
                'scale': '240',
                'ma': 'no',
                'count': '30'
            }
            
            # æ„é€ headers - ä½¿ç”¨é¡µé¢è¯·æ±‚çš„æ‰€æœ‰headers
            headers = self.headers.copy()
            
            # æ·»åŠ cookies
            if self.cookies:
                cookie_str = '; '.join([f"{k}={v}" for k, v in self.cookies.items()])
                headers['Cookie'] = cookie_str
            
            print(f"  ğŸ“¡ è¯·æ±‚API: {api_url}")
            print(f"  ğŸ“‹ è¯·æ±‚å‚æ•°: {params}")
            print(f"  ğŸ“‹ è¯·æ±‚headers: {len(headers)} ä¸ª")
            
            response = self.session.get(api_url, params=params, headers=headers, timeout=15)
            
            print(f"  ğŸ“Š çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    print(f"  ğŸ“„ å“åº”æ•°æ®: {len(data) if isinstance(data, list) else 'dict'} æ¡")
                    
                    return self._parse_api_history_data(data, target_date)
                except json.JSONDecodeError as e:
                    print(f"  âŒ JSONè§£æå¤±è´¥: {e}")
                    print(f"  ğŸ“„ å“åº”å†…å®¹: {response.text[:200]}")
                    return None
            else:
                print(f"  âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"  ğŸ“„ å“åº”å†…å®¹: {response.text[:200]}")
                return None
                
        except Exception as e:
            print(f"  âŒ è·å–å†å²æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _parse_api_history_data(self, api_data: list, target_date: str) -> Optional[Dict]:
        """è§£æAPIå†å²æ•°æ®"""
        if not api_data:
            return None
        
        target_datetime = datetime.strptime(target_date, "%Y-%m-%d")
        
        for item in api_data:
            try:
                item_date = datetime.strptime(item['day'], "%Y-%m-%d")
                if item_date.date() == target_datetime.date():
                    return {
                        "date": target_date,
                        "open": float(item['open']),
                        "high": float(item['high']),
                        "low": float(item['low']),
                        "close": float(item['close']),
                        "volume": int(item['volume']),
                    }
            except (KeyError, ValueError) as e:
                print(f"  âš ï¸  è§£ææ•°æ®é¡¹å¤±è´¥: {e}")
                continue
        
        return None


class SinaStockCrawler:
    """æ–°æµªè´¢ç»è‚¡ç¥¨çˆ¬è™« - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, db_path: str = None):
        self.fetcher = SinaHistoryFetcher()
        self.db_path = db_path
    
    async def crawl_stock_price(self, stock_code: str, target_date: str = None) -> Optional[Dict]:
        """çˆ¬å–å•ä¸ªè‚¡ç¥¨ä»·æ ¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        print(f"ğŸ•·ï¸ çˆ¬å–æ–°æµªè´¢ç» {stock_code}")
        print(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
        
        try:
            if target_date:
                # åˆå§‹åŒ–ä¼šè¯ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
                if not self.fetcher.session_initialized:
                    success = await self.fetcher.init_session(stock_code)
                    if not success:
                        print(f"âŒ ä¼šè¯åˆå§‹åŒ–å¤±è´¥")
                        return None
                    await asyncio.sleep(2)
                
                # ä½¿ç”¨ä¼šè¯è·å–å†å²æ•°æ®
                data = self.fetcher.fetch_history_data(stock_code, target_date)
                
                if data:
                    data["code"] = stock_code
                    print(f"âœ… æˆåŠŸä»APIæå– {stock_code}: ä»·æ ¼ {data.get('close')} å…ƒ")
                    return data
                else:
                    print(f"âš ï¸  APIæœªæ‰¾åˆ° {stock_code} åœ¨ {target_date} çš„æ•°æ®")
                    return None
            else:
                print(f"âš ï¸  å®æ—¶æ•°æ®æ¨¡å¼æš‚ä¸æ”¯æŒ")
                return None
                
        except Exception as e:
            print(f"âŒ çˆ¬å– {stock_code} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None


async def test_optimized_crawler():
    """æµ‹è¯•ä¼˜åŒ–åçš„çˆ¬è™«"""
    db_path = "/Users/riching/work/hywork/db/sqlite/full_a_stock_cache.db"
    test_stocks = ["000001", "600519", "000858"]
    target_date = "2026-02-09"
    
    print("=" * 60)
    print("æµ‹è¯•ä¼˜åŒ–åçš„æ–°æµªçˆ¬è™« - ä½¿ç”¨å®Œæ•´headers")
    print("=" * 60)
    print(f"ç›®æ ‡æ—¥æœŸ: {target_date}")
    print(f"æµ‹è¯•è‚¡ç¥¨: {test_stocks}")
    
    crawler = SinaStockCrawler(db_path)
    
    try:
        results = []
        
        for i, stock_code in enumerate(test_stocks):
            print(f"\n{'='*60}")
            print(f"æµ‹è¯•ç¬¬ {i+1}/{len(test_stocks)} åªè‚¡ç¥¨: {stock_code}")
            print(f"{'='*60}")
            
            data = await crawler.crawl_stock_price(stock_code, target_date)
            if data:
                results.append(data)
            
            await asyncio.sleep(1)
        
        print(f"\n{'='*60}")
        print("æµ‹è¯•ç»“æœæ±‡æ€»:")
        print(f"{'='*60}")
        print(f"æˆåŠŸçˆ¬å–: {len(results)} åªè‚¡ç¥¨")
        for result in results:
            print(f"- {result['code']} ({result['date']}): {result['close']} å…ƒ")
        
        return len(results) > 0
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(test_optimized_crawler())
    sys.exit(0 if success else 1)
