#!/usr/bin/env python3
"""ä¸œæ–¹è´¢å¯Œè‚¡ç¥¨ä»·æ ¼çˆ¬è™« - å®Œæ•´ç‰ˆ"""

import asyncio
import sys
import os
import sqlite3
import re
from datetime import datetime
from typing import Dict, List, Optional
from playwright.async_api import async_playwright

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from crawler.sina.market_prefix_helper import get_market_prefix

class EastMoneyStockExtractor:
    """ä¸œæ–¹è´¢å¯Œè‚¡ç¥¨æ•°æ®æå–å™¨"""
    
    def __init__(self):
        self.name = "EastMoney"
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]

    async def extract_stock_data(self, page, stock_code: str) -> Optional[Dict]:
        """æå–ä¸œæ–¹è´¢å¯Œè‚¡ç¥¨æ•°æ®"""
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            await page.wait_for_load_state("domcontentloaded", timeout=15000)
            await page.wait_for_timeout(5000)  # é¢å¤–ç­‰å¾…ç¡®ä¿æ•°æ®åŠ è½½
            
            # è§£æè‚¡ç¥¨æ•°æ®
            stock_data = await self._parse_eastmoney_data(page, stock_code)
            return stock_data
            
        except Exception as e:
            print(f"ä¸œæ–¹è´¢å¯Œæå–é”™è¯¯ {stock_code}: {e}")
            return None

    async def _parse_eastmoney_data(self, page, stock_code: str) -> Optional[Dict]:
        """è§£æä¸œæ–¹è´¢å¯Œé¡µé¢å†…å®¹ - ä»é¡µé¢å…ƒç´ ä¸­æå–ä»·æ ¼ä¿¡æ¯"""
        data = {
            "code": stock_code,
            "name": None,
            "date": datetime.now().strftime("%Y-%m-%d"),
            "open": None,
            "high": None,
            "low": None,
            "close": None,
            "volume": None,
            "change": None,
            "change_percent": None,
        }

        # æå–è‚¡ç¥¨åç§°ï¼ˆä»é¡µé¢å…ƒç´ ï¼‰
        # æŸ¥æ‰¾è‚¡ç¥¨åç§°å…ƒç´ 
        name_element = await page.query_selector('.stock-name, [class*="name"]')
        if name_element:
            name_text = await name_element.inner_text()
            # å»æ‰æ‰€æœ‰ç©ºæ ¼
            data["name"] = name_text.replace(' ', '').strip()
            print(f"  ğŸ“„ ä»å…ƒç´ æå–åç§°: {data['name']}")
        else:
            # å¤‡é€‰æ–¹æ¡ˆï¼šä»æ ‡é¢˜æå–
            html = await page.content()
            # æ ‡é¢˜æ ¼å¼: <title>å¹³å®‰é“¶è¡Œ 10.91 -0.05(-0.46%)è‚¡ç¥¨ä»·æ ¼_è¡Œæƒ…_èµ°åŠ¿å›¾â€”ä¸œæ–¹è´¢å¯Œç½‘</title>
            # æ ‡é¢˜æ ¼å¼: <title>äº” ç²® æ¶² 106.06 1.44... </title>
            title_match = re.search(r'<title>([^0-9]+)', html)
            if title_match:
                data["name"] = title_match.group(1).replace(' ', '').strip()

        # ä»é¡µé¢å…ƒç´ ä¸­æå–ä»·æ ¼æ•°æ®
        # ä½¿ç”¨XPathå®šä½ä»·æ ¼ä¿¡æ¯è¡¨æ ¼
        try:
            # æŸ¥æ‰¾åŒ…å«"ä»Šå¼€ï¼š"çš„tdå…ƒç´ 
            open_element = await page.query_selector('xpath=//td[contains(text(), "ä»Šå¼€ï¼š")]/following-sibling::td//span//span')
            if open_element:
                open_text = await open_element.inner_text()
                try:
                    data["open"] = float(open_text)
                    print(f"  ğŸ“„ ä»Šå¼€: {open_text}")
                except ValueError:
                    pass
            
            # æŸ¥æ‰¾åŒ…å«"æœ€é«˜ï¼š"çš„tdå…ƒç´ 
            high_element = await page.query_selector('xpath=//td[contains(text(), "æœ€é«˜ï¼š")]/following-sibling::td//span//span')
            if high_element:
                high_text = await high_element.inner_text()
                try:
                    data["high"] = float(high_text)
                    print(f"  ğŸ“„ æœ€é«˜: {high_text}")
                except ValueError:
                    pass
            
            # æŸ¥æ‰¾åŒ…å«"æœ€ä½ï¼š"çš„tdå…ƒç´ 
            low_element = await page.query_selector('xpath=//td[contains(text(), "æœ€ä½ï¼š")]/following-sibling::td//span//span')
            if low_element:
                low_text = await low_element.inner_text()
                try:
                    data["low"] = float(low_text)
                    print(f"  ğŸ“„ æœ€ä½: {low_text}")
                except ValueError:
                    pass
            
            # æŸ¥æ‰¾åŒ…å«"æ˜¨æ”¶ï¼š"çš„tdå…ƒç´ 
            close_element = await page.query_selector('xpath=//td[contains(text(), "æ˜¨æ”¶ï¼š")]/following-sibling::td//span//span')
            if close_element:
                close_text = await close_element.inner_text()
                try:
                    data["close"] = float(close_text)
                    print(f"  ğŸ“„ æ˜¨æ”¶: {close_text}")
                except ValueError:
                    pass
            
            # æŸ¥æ‰¾åŒ…å«"æˆäº¤é‡ï¼š"çš„tdå…ƒç´ 
            volume_element = await page.query_selector('xpath=//td[contains(text(), "æˆäº¤é‡ï¼š")]/following-sibling::td//span//span')
            if volume_element:
                volume_text = await volume_element.inner_text()
                try:
                    if 'ä¸‡' in volume_text:
                        data["volume"] = int(float(volume_text.replace('ä¸‡', '').replace('äº¿', '')) * 10000)
                    elif 'äº¿' in volume_text:
                        data["volume"] = int(float(volume_text.replace('äº¿', '')) * 100000000)
                    else:
                        data["volume"] = int(float(volume_text))
                    print(f"  ğŸ“„ æˆäº¤é‡: {volume_text}")
                except ValueError:
                    pass
            
            # æŸ¥æ‰¾åŒ…å«"æ¢æ‰‹ï¼š"çš„tdå…ƒç´ ï¼ˆæ¶¨è·Œå¹…ï¼‰
            change_element = await page.query_selector('xpath=//td[contains(text(), "æ¢æ‰‹ï¼š")]/following-sibling::td//span//span')
            if change_element:
                change_text = await change_element.inner_text()
                try:
                    if '%' in change_text:
                        data["change_percent"] = float(change_text.replace('%', ''))
                        print(f"  ğŸ“„ æ¢æ‰‹(æ¶¨è·Œå¹…): {change_text}")
                except ValueError:
                    pass
            
            # è®¡ç®—æ¶¨è·Œé¢
            if data["open"] and data["close"]:
                data["change"] = data["close"] - data["open"]
                
        except Exception as e:
            print(f"  âš ï¸  ä»è¡¨æ ¼æå–æ•°æ®å¤±è´¥: {e}")

        # ç¡®ä¿è‡³å°‘æœ‰æ”¶ç›˜ä»·æ‰èƒ½è¿”å›æ•°æ®
        if data["close"] is not None:
            return data
        return None

class EastMoneyStockCrawler:
    """ä¸œæ–¹è´¢å¯Œè‚¡ç¥¨çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self, db_path: str = None):
        self.extractor = EastMoneyStockExtractor()
        self.browser = None
        self.playwright = None
        self.db_path = db_path

    async def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--disable-web-security",
            ],
        )

    async def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()

    def get_eastmoney_url(self, stock_code: str) -> str:
        """è·å–ä¸œæ–¹è´¢å¯Œè‚¡ç¥¨URL"""
        market_prefix = get_market_prefix(stock_code)
        return f"https://quote.eastmoney.com/{market_prefix}{stock_code}.html"

    async def crawl_stock_price(self, stock_code: str) -> Optional[Dict]:
        """çˆ¬å–å•ä¸ªè‚¡ç¥¨ä»·æ ¼"""
        url = self.get_eastmoney_url(stock_code)
        print(f"ğŸ•·ï¸ çˆ¬å–ä¸œæ–¹è´¢å¯Œ {stock_code}: {url}")
        
        try:
            page = await self.browser.new_page()
            await page.set_extra_http_headers({
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            })
            
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            
            # æå–æ•°æ®
            data = await self.extractor.extract_stock_data(page, stock_code)
            
            await page.close()
            
            if data:
                print(f"âœ… æˆåŠŸæå– {stock_code}: ä»·æ ¼ {data.get('close')} å…ƒ")
                return data
            else:
                print(f"âŒ æœªèƒ½æå– {stock_code} çš„æ•°æ®")
                return None
                
        except Exception as e:
            print(f"âŒ çˆ¬å– {stock_code} å¤±è´¥: {e}")
            return None

    def save_to_database(self, stock_data_list: List[Dict]):
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
        if not self.db_path or not stock_data_list:
            return
            
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for data in stock_data_list:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ—¥æœŸçš„æ•°æ®
                cursor.execute(
                    "SELECT COUNT(*) FROM merged_stocks WHERE code = ? AND date = ?",
                    (data["code"], data["date"])
                )
                if cursor.fetchone()[0] > 0:
                    print(f"âš ï¸  {data['code']} {data['date']} æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    continue
                
                # æ’å…¥æ–°æ•°æ®
                insert_data = (
                    None,  # id
                    datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # created_at
                    data["code"],
                    data["date"],
                    data.get("open"),
                    data.get("high"),
                    data.get("low"),
                    data.get("close"),
                    data.get("volume"),
                    None,  # amount
                    None,  # outstanding_share
                    None,  # turnover
                    data.get("name"),
                    None,  # ma5
                    None,  # ma10
                    None,  # ma20
                    None,  # rsi6
                    None,  # rsi14
                    data.get("change_percent"),  # pct_change
                )
                
                cursor.execute("""
                    INSERT INTO merged_stocks 
                    (id, created_at, code, date, open, high, low, close, volume, amount, 
                     outstanding_share, turnover, name, ma5, ma10, ma20, rsi6, rsi14, pct_change)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, insert_data)
                
                # æ›´æ–°æ•°æ®çŠ¶æ€
                cursor.execute("""
                    INSERT OR REPLACE INTO data_status 
                    (code, last_updated, record_count, status)
                    VALUES (?, ?, ?, ?)
                """, (data["code"], datetime.now().isoformat(), 1, "success"))
            
            conn.commit()
            conn.close()
            print(f"âœ… æˆåŠŸä¿å­˜ {len(stock_data_list)} æ¡è®°å½•åˆ°æ•°æ®åº“")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")

async def crawl_multiple_stocks(stock_codes: List[str], db_path: str = None):
    """æ‰¹é‡çˆ¬å–å¤šä¸ªè‚¡ç¥¨"""
    crawler = EastMoneyStockCrawler(db_path)
    
    try:
        await crawler.init_browser()
        
        results = []
        for i, stock_code in enumerate(stock_codes):
            print(f"\n--- å¤„ç†ç¬¬ {i+1}/{len(stock_codes)} åªè‚¡ç¥¨ ---")
            data = await crawler.crawl_stock_price(stock_code)
            if data:
                results.append(data)
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(stock_codes) - 1:
                await asyncio.sleep(2)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        if results and db_path:
            crawler.save_to_database(results)
        
        print(f"\nğŸ“Š çˆ¬å–ç»“æœæ±‡æ€»:")
        print(f"æˆåŠŸçˆ¬å–: {len(results)} åªè‚¡ç¥¨")
        for result in results:
            print(f"- {result['code']} ({result['name']}): {result['close']} å…ƒ")
            
        return len(results)
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
        return 0
    finally:
        await crawler.close_browser()

async def main():
    """ä¸»å‡½æ•° - æµ‹è¯•æ¨¡å¼"""
    # æ•°æ®åº“è·¯å¾„ï¼ˆå¦‚æœéœ€è¦ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
    db_path = "/Users/riching/work/hywork/db/sqlite/full_a_stock_cache.db"
    
    # æµ‹è¯•è‚¡ç¥¨ä»£ç 
    test_stocks = ["002323", "000001", "600519", "000858", "600036"]
    
    success_count = await crawl_multiple_stocks(test_stocks, db_path)
    
    if success_count > 0:
        print(f"\nğŸ‰ æˆåŠŸçˆ¬å– {success_count} åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®ï¼")
        return True
    else:
        print(f"\nâŒ æœªèƒ½æˆåŠŸçˆ¬å–ä»»ä½•è‚¡ç¥¨æ•°æ®")
        return False

if __name__ == "__main__":
    import re  # ç¡®ä¿å¯¼å…¥reæ¨¡å—
    success = asyncio.run(main())
    sys.exit(0 if success else 1)